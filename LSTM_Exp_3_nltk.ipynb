{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0902760e-b4eb-4525-aec0-58656ccf5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-05-21 10:52:23.756092: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 10:52:24.559106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-21 10:52:24.559134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-21 10:52:24.562555: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-21 10:52:24.681871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from Experiment_Template import performExperimentLSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086bd725-91e0-4db0-bc03-397b2e11c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rng instance\n",
    "seed = 13\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9f57da-c46c-47e0-941c-47e141a0232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general parameters\n",
    "min_lr = 0.000001\n",
    "experiment_dir = './Exp3_LSTM'\n",
    "log_file = 'training_log_Exp_3_LSTM.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d2b184-93b6-4de8-a590-3c41ed1519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scientic parameter\n",
    "tokenization_level = 'word'\n",
    "embedding_type = 'glove'\n",
    "tokenization_type='nltk_shakespeare'\n",
    "\n",
    "# Define some fixed parameters\n",
    "seq_length = 50\n",
    "n_layers = 2\n",
    "dim_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa4faaf-2e6c-4aa3-9b33-d2c7b2475d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.01310716621823221\n",
      "Fine tune embedding: False\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT SET THE TRIAL!!!!!\n",
    "trial = 1\n",
    "\n",
    "# Define nuisance parameters\n",
    "# init_lr = 0.001\n",
    "max_init_lr = 0.015\n",
    "min_init_lr = 0.001\n",
    "init_lr = (max_init_lr - min_init_lr) * rng.random() + min_init_lr\n",
    "print(f'Learning rate: {init_lr}')\n",
    "\n",
    "\n",
    "fine_tune_embedding = False\n",
    "print(f'Fine tune embedding: {fine_tune_embedding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9290db69-4b31-4fe6-87c4-4e90737d8d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in: /home/jovyan/DD2417_project/Project_1/DLIDS_project\n",
      "Running on cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model1, history1, vocab1, train_dataset1, val_dataset1 \u001b[38;5;241m=\u001b[39m \u001b[43mperformExperimentLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_hidden\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenization_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenization_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenization_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenization_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfine_tune_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfine_tune_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minit_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DD2417_project/Project_1/DLIDS_project/Experiment_Template.py:139\u001b[0m, in \u001b[0;36mperformExperimentLSTM\u001b[0;34m(dim_hidden, n_layers, tokenization_level, tokenization_type, embedding_type, fine_tune_embedding, seq_length, init_lr, min_lr, trial, experiment_dir, log_file)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear decay is not implemented yet.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m history, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersistent_hidden_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_state_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_states_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history, vocab, train_dataset, val_dataset\n",
      "File \u001b[0;32m~/DD2417_project/Project_1/DLIDS_project/LSTM/LSTM.py:85\u001b[0m, in \u001b[0;36mtrain_lstm\u001b[0;34m(model, dataloader_train, dataloader_val, optimizer, persistent_hidden_state, hidden_state, cell_state, hidden_state_val, cell_state_val, device, num_epochs, print_every, val_every_n_steps, scheduler, experiment_dir, log_file, trial, resume_training_epoch, resume_checkpoint_file)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_lstm\u001b[39m(model, dataloader_train, dataloader_val, optimizer, persistent_hidden_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, hidden_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cell_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hidden_state_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cell_state_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     77\u001b[0m               print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, val_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, experiment_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Baseline_LSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     78\u001b[0m               log_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_log.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, resume_training_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, resume_checkpoint_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Little sanity checks, i.e. if we use a persistent hidden state the dataset\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# across the buckets must be the same and we extract information from them and read out data\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m persistent_hidden_state:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# Move hidden_state and cell_state to appropriate device\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m         hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m         hidden_state_val \u001b[38;5;241m=\u001b[39m hidden_state_val\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     87\u001b[0m         cell_state \u001b[38;5;241m=\u001b[39m cell_state\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model1, history1, vocab1, train_dataset1, val_dataset1 = performExperimentLSTM(dim_hidden = dim_hidden, n_layers= n_layers, tokenization_level=tokenization_level, tokenization_type=tokenization_type, embedding_type =embedding_type, \n",
    "                         fine_tune_embedding = fine_tune_embedding, seq_length = seq_length, init_lr= init_lr, min_lr = min_lr, \n",
    "                         trial=trial, experiment_dir = experiment_dir, log_file = log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b378b-bbbd-4c2c-a689-aaa205503df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT SET THE TRIAL!!!!!\n",
    "trial = 2\n",
    "\n",
    "# Define nuisance parameters\n",
    "# init_lr = 0.001\n",
    "max_init_lr = 0.015\n",
    "min_init_lr = 0.001\n",
    "init_lr = (max_init_lr - min_init_lr) * rng.random() + min_init_lr\n",
    "print(f'Learning rate: {init_lr}')\n",
    "\n",
    "\n",
    "fine_tune_embedding = False\n",
    "print(f'Fine tune embedding: {fine_tune_embedding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4341855-9fe1-49ff-afa0-82e02bb5a63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2, history2, vocab2, train_dataset2, val_dataset2 = performExperimentLSTM(dim_hidden = dim_hidden, n_layers= n_layers, tokenization_level=tokenization_level, tokenization_type=tokenization_type, embedding_type =embedding_type, \n",
    "                         fine_tune_embedding = fine_tune_embedding, seq_length = seq_length, init_lr= init_lr, min_lr = min_lr, \n",
    "                         trial=trial, experiment_dir = experiment_dir, log_file = log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e2720-a684-4aab-b3e0-26e172e46e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT SET THE TRIAL!!!!!\n",
    "trial = 3\n",
    "\n",
    "# Define nuisance parameters\n",
    "# init_lr = 0.001\n",
    "max_init_lr = 0.015\n",
    "min_init_lr = 0.001\n",
    "init_lr = (max_init_lr - min_init_lr) * rng.random() + min_init_lr\n",
    "print(f'Learning rate: {init_lr}')\n",
    "\n",
    "\n",
    "fine_tune_embedding = True\n",
    "print(f'Fine tune embedding: {fine_tune_embedding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f1b6b-0069-4bf8-bb34-4da56049f206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3, history3, vocab3, train_dataset3, val_dataset3 = performExperimentLSTM(dim_hidden = dim_hidden, n_layers= n_layers, tokenization_level=tokenization_level, tokenization_type=tokenization_type, embedding_type =embedding_type, \n",
    "                         fine_tune_embedding = fine_tune_embedding, seq_length = seq_length, init_lr= init_lr, min_lr = min_lr, \n",
    "                         trial=trial, experiment_dir = experiment_dir, log_file = log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc65062-3371-458d-a09a-f73fa54e3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT SET THE TRIAL!!!!!\n",
    "trial = 4\n",
    "\n",
    "# Define nuisance parameters\n",
    "# init_lr = 0.001\n",
    "max_init_lr = 0.015\n",
    "min_init_lr = 0.001\n",
    "init_lr = (max_init_lr - min_init_lr) * rng.random() + min_init_lr\n",
    "print(f'Learning rate: {init_lr}')\n",
    "\n",
    "\n",
    "fine_tune_embedding = True\n",
    "print(f'Fine tune embedding: {fine_tune_embedding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc2df3-6790-4dce-904b-d0aeac5bf23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model4, history4, vocab4, train_dataset4, val_dataset4 = performExperimentLSTM(dim_hidden = dim_hidden, n_layers= n_layers, tokenization_level=tokenization_level, tokenization_type=tokenization_type, embedding_type =embedding_type, \n",
    "                         fine_tune_embedding = fine_tune_embedding, seq_length = seq_length, init_lr= init_lr, min_lr = min_lr, \n",
    "                         trial=trial, experiment_dir = experiment_dir, log_file = log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8767c-23c2-4049-bf09-c639f1cbe9da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
